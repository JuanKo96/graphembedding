{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph.wiki import get_encodings\n",
    "from rankingmodel.data_preprocess import DataPreprocess\n",
    "from rankingmodel.models import TemporalSAGE\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/nasdaq_test_price.pickle', 'rb') as f:\n",
    "    nasdaq_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AABA</th>\n",
       "      <th>AAON</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>AAWW</th>\n",
       "      <th>AAXJ</th>\n",
       "      <th>ABAX</th>\n",
       "      <th>ABCB</th>\n",
       "      <th>ABMD</th>\n",
       "      <th>ACGL</th>\n",
       "      <th>ACHC</th>\n",
       "      <th>...</th>\n",
       "      <th>JAZZ</th>\n",
       "      <th>JBHT</th>\n",
       "      <th>JBLU</th>\n",
       "      <th>JBSS</th>\n",
       "      <th>JCOM</th>\n",
       "      <th>JJSF</th>\n",
       "      <th>JKHY</th>\n",
       "      <th>JKI</th>\n",
       "      <th>JMBA</th>\n",
       "      <th>JOBS</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-01-02</th>\n",
       "      <td>5.516981</td>\n",
       "      <td>8.894916</td>\n",
       "      <td>17.011521</td>\n",
       "      <td>45.58</td>\n",
       "      <td>53.233575</td>\n",
       "      <td>36.289190</td>\n",
       "      <td>11.861925</td>\n",
       "      <td>13.6900</td>\n",
       "      <td>14.793333</td>\n",
       "      <td>24.3700</td>\n",
       "      <td>...</td>\n",
       "      <td>54.58</td>\n",
       "      <td>56.432451</td>\n",
       "      <td>5.90</td>\n",
       "      <td>13.594767</td>\n",
       "      <td>27.844603</td>\n",
       "      <td>57.646466</td>\n",
       "      <td>36.760707</td>\n",
       "      <td>23.157976</td>\n",
       "      <td>11.750</td>\n",
       "      <td>24.050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-03</th>\n",
       "      <td>5.434556</td>\n",
       "      <td>9.049646</td>\n",
       "      <td>16.796670</td>\n",
       "      <td>44.76</td>\n",
       "      <td>52.828462</td>\n",
       "      <td>36.193238</td>\n",
       "      <td>11.824944</td>\n",
       "      <td>13.4500</td>\n",
       "      <td>14.750000</td>\n",
       "      <td>24.1900</td>\n",
       "      <td>...</td>\n",
       "      <td>55.21</td>\n",
       "      <td>56.635844</td>\n",
       "      <td>5.95</td>\n",
       "      <td>13.601933</td>\n",
       "      <td>27.757014</td>\n",
       "      <td>57.969114</td>\n",
       "      <td>36.814701</td>\n",
       "      <td>23.229510</td>\n",
       "      <td>11.750</td>\n",
       "      <td>24.315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-04</th>\n",
       "      <td>5.456536</td>\n",
       "      <td>9.095647</td>\n",
       "      <td>16.328928</td>\n",
       "      <td>45.19</td>\n",
       "      <td>52.828462</td>\n",
       "      <td>36.135666</td>\n",
       "      <td>11.926644</td>\n",
       "      <td>13.4100</td>\n",
       "      <td>14.876667</td>\n",
       "      <td>23.6300</td>\n",
       "      <td>...</td>\n",
       "      <td>55.62</td>\n",
       "      <td>57.209044</td>\n",
       "      <td>5.95</td>\n",
       "      <td>13.530268</td>\n",
       "      <td>27.923434</td>\n",
       "      <td>58.049776</td>\n",
       "      <td>36.895691</td>\n",
       "      <td>23.434664</td>\n",
       "      <td>12.400</td>\n",
       "      <td>26.510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-07</th>\n",
       "      <td>5.330151</td>\n",
       "      <td>8.995281</td>\n",
       "      <td>16.232875</td>\n",
       "      <td>44.84</td>\n",
       "      <td>52.414729</td>\n",
       "      <td>35.962952</td>\n",
       "      <td>11.899924</td>\n",
       "      <td>13.2495</td>\n",
       "      <td>14.730000</td>\n",
       "      <td>24.1500</td>\n",
       "      <td>...</td>\n",
       "      <td>55.50</td>\n",
       "      <td>56.783767</td>\n",
       "      <td>5.97</td>\n",
       "      <td>13.644932</td>\n",
       "      <td>27.809568</td>\n",
       "      <td>56.822548</td>\n",
       "      <td>36.958684</td>\n",
       "      <td>23.316236</td>\n",
       "      <td>12.499</td>\n",
       "      <td>26.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-08</th>\n",
       "      <td>5.401586</td>\n",
       "      <td>9.016191</td>\n",
       "      <td>16.276564</td>\n",
       "      <td>45.53</td>\n",
       "      <td>51.975139</td>\n",
       "      <td>36.452309</td>\n",
       "      <td>11.926644</td>\n",
       "      <td>13.2100</td>\n",
       "      <td>14.750000</td>\n",
       "      <td>24.9392</td>\n",
       "      <td>...</td>\n",
       "      <td>56.01</td>\n",
       "      <td>56.700561</td>\n",
       "      <td>5.94</td>\n",
       "      <td>13.609099</td>\n",
       "      <td>27.888398</td>\n",
       "      <td>56.848808</td>\n",
       "      <td>36.787704</td>\n",
       "      <td>23.197095</td>\n",
       "      <td>12.700</td>\n",
       "      <td>26.255</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 481 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                AABA      AAON       AAPL   AAWW       AAXJ       ABAX  \\\n",
       "date                                                                     \n",
       "2013-01-02  5.516981  8.894916  17.011521  45.58  53.233575  36.289190   \n",
       "2013-01-03  5.434556  9.049646  16.796670  44.76  52.828462  36.193238   \n",
       "2013-01-04  5.456536  9.095647  16.328928  45.19  52.828462  36.135666   \n",
       "2013-01-07  5.330151  8.995281  16.232875  44.84  52.414729  35.962952   \n",
       "2013-01-08  5.401586  9.016191  16.276564  45.53  51.975139  36.452309   \n",
       "\n",
       "                 ABCB     ABMD       ACGL     ACHC  ...   JAZZ       JBHT  \\\n",
       "date                                                ...                     \n",
       "2013-01-02  11.861925  13.6900  14.793333  24.3700  ...  54.58  56.432451   \n",
       "2013-01-03  11.824944  13.4500  14.750000  24.1900  ...  55.21  56.635844   \n",
       "2013-01-04  11.926644  13.4100  14.876667  23.6300  ...  55.62  57.209044   \n",
       "2013-01-07  11.899924  13.2495  14.730000  24.1500  ...  55.50  56.783767   \n",
       "2013-01-08  11.926644  13.2100  14.750000  24.9392  ...  56.01  56.700561   \n",
       "\n",
       "            JBLU       JBSS       JCOM       JJSF       JKHY        JKI  \\\n",
       "date                                                                      \n",
       "2013-01-02  5.90  13.594767  27.844603  57.646466  36.760707  23.157976   \n",
       "2013-01-03  5.95  13.601933  27.757014  57.969114  36.814701  23.229510   \n",
       "2013-01-04  5.95  13.530268  27.923434  58.049776  36.895691  23.434664   \n",
       "2013-01-07  5.97  13.644932  27.809568  56.822548  36.958684  23.316236   \n",
       "2013-01-08  5.94  13.609099  27.888398  56.848808  36.787704  23.197095   \n",
       "\n",
       "              JMBA    JOBS  \n",
       "date                        \n",
       "2013-01-02  11.750  24.050  \n",
       "2013-01-03  11.750  24.315  \n",
       "2013-01-04  12.400  26.510  \n",
       "2013-01-07  12.499  26.400  \n",
       "2013-01-08  12.700  26.255  \n",
       "\n",
       "[5 rows x 481 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nasdaq_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DP = DataPreprocess()\n",
    "window_size = 30\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = DP.get_data(nasdaq_data, window_size, split_ratio=(0.6,0.2,0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:  (481, 30, 736)\n",
      "y_train shape:  (481, 736)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape: \", X_train.shape)\n",
    "print(\"y_train shape: \", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tickers = nasdaq_data.columns.tolist()\n",
    "encoding, binary_encoding = get_encodings('20180105', 'NASDAQ', model_tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding shape:  (481, 481, 43)\n",
      "Binary encoding shape:,  (481, 481)\n"
     ]
    }
   ],
   "source": [
    "print(\"Encoding shape: \", encoding.shape)\n",
    "print(\"Binary encoding shape:, \", binary_encoding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 736/736 [00:00<00:00, 11153.25it/s]\n",
      "100%|██████████| 246/246 [00:00<00:00, 93740.24it/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "y_train_tensor = torch.FloatTensor(y_train)\n",
    "\n",
    "train_data = []\n",
    "for i in tqdm(range(X_train_tensor.shape[2])):\n",
    "    train_data.append([X_train_tensor[:,:,i], y_train_tensor[:,i]])\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "X_valid_tensor = torch.FloatTensor(X_val)\n",
    "y_valid_tensor = torch.FloatTensor(y_val)\n",
    "\n",
    "valid_data = []\n",
    "for i in tqdm(range(X_valid_tensor.shape[2])):\n",
    "    valid_data.append([X_valid_tensor[:,:,i], y_valid_tensor[:,i]])\n",
    "valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=batch_size, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 'cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda = torch.cuda.is_available() \n",
    "if cuda:\n",
    "    device = 'cuda:0'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "cuda, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Purpose\n",
    "seq_embed_size = 64\n",
    "input_size = X_train.shape[2]\n",
    "\n",
    "\n",
    "input_data = torch.FloatTensor(X_train).to(device)\n",
    "rel_encoding = torch.FloatTensor(encoding).to(device)\n",
    "\n",
    "\n",
    "model = TemporalSAGE(\n",
    "    input_size=input_size, \n",
    "    seq_embed_size=seq_embed_size, \n",
    "    relational_encoding=rel_encoding, \n",
    "    k_hops=3, \n",
    "    hop_layers=2, \n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TemporalSAGE(\n",
      "  (sequential_embedding_model): SequentialEmbedding(\n",
      "    (lstm): LSTM(736, 64, num_layers=2)\n",
      "  )\n",
      "  (relational_embedding_model): RelationalEmbedding(\n",
      "    (softmax_dim_1): Softmax(dim=1)\n",
      "    (activation_function): LeakyReLU(negative_slope=0.01)\n",
      "    (linear_1_hop_layer_1): Linear(in_features=171, out_features=171, bias=True)\n",
      "    (linear_1_hop_layer_2): Linear(in_features=171, out_features=1, bias=True)\n",
      "    (linear_2_hop_layer_1): Linear(in_features=171, out_features=171, bias=True)\n",
      "    (linear_2_hop_layer_2): Linear(in_features=171, out_features=1, bias=True)\n",
      "    (linear_3_hop_layer_1): Linear(in_features=171, out_features=171, bias=True)\n",
      "    (linear_3_hop_layer_2): Linear(in_features=171, out_features=1, bias=True)\n",
      "  )\n",
      "  (combined_prediction_model): FullyConnected(\n",
      "    (activation_function): LeakyReLU(negative_slope=0.01)\n",
      "    (linear_layer_1): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (linear_layer_2): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (linear_layer_3): Linear(in_features=128, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/LSTM_64_seq_embed_size_30_window\n"
     ]
    }
   ],
   "source": [
    "path = f'models/LSTM_{seq_embed_size}_seq_embed_size_{window_size}_window'\n",
    "print(path)\n",
    "\n",
    "num_epoch = int(1e4)\n",
    "lr = 1e-4\n",
    "weight_decay = 1e-8\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of scalar type Long but got scalar type Float for argument #2 'target' in call to _thnn_nll_loss_forward",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    959\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 961\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    962\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2460\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2461\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2255\u001b[0m                          .format(input.size(0), target.size(0)))\n\u001b[1;32m   2256\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2257\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2258\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2259\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of scalar type Long but got scalar type Float for argument #2 'target' in call to _thnn_nll_loss_forward"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "loss_array_train = []\n",
    "loss_array_valid = []\n",
    "patience = 0\n",
    "min_loss = np.inf\n",
    "\n",
    "for e in range(num_epoch):\n",
    "    loss_array_train_tmp = []\n",
    "    model.train()\n",
    "    \n",
    "#     for X_train_batch, Y_train_batch in train_loader:\n",
    "        \n",
    "    out = model(X_train_tensor.to(device))\n",
    "\n",
    "    loss = criterion(out, y_train_tensor.to(device))\n",
    "    loss_array_train_tmp.append(loss.item())\n",
    "\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    torch.nn.utils.clip_grad_value_(model.parameters(), 1.)\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    loss_array_train.append(np.mean(loss_array_train_tmp))\n",
    "        \n",
    "    torch.cuda.empty_cache() ## 캐시 비워주기 자주 해줘야함\n",
    "    \n",
    "    loss_array_valid_tmp = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "#         for X_train_batch, Y_train_batch in valid_loader:\n",
    "            \n",
    "        out = model(X_valid_tensor.to(device))\n",
    "\n",
    "        loss = criterion(out, y_valid_tensor.to(device))\n",
    "        loss_array_valid_tmp.append(loss.item())\n",
    "\n",
    "        loss_array_valid.append(np.mean(loss_array_valid_tmp))\n",
    "\n",
    "    if e % 100 == 0: \n",
    "        print('Epoch: {}, Train Loss: {:.4e}, Valid Loss: {:.4e}'.format(e, loss_array_train[-1], loss_array_valid[-1]))\n",
    "\n",
    "    ## update the minimum loss\n",
    "    if min_loss > loss_array_train[-1]:\n",
    "        patience = 0\n",
    "        min_loss = loss_array_train[-1]\n",
    "        torch.save(model.state_dict(), path)\n",
    "    else:\n",
    "        patience += 1\n",
    "\n",
    "    ## early stop when patience become larger than 10\n",
    "    if patience > 10:\n",
    "        break\n",
    "        \n",
    "    torch.cuda.empty_cache() ## 캐시 비워주기 자주 해줘야함\n",
    "    \n",
    "\n",
    "plt.plot(loss_array_train, label='Train Loss')\n",
    "plt.plot(loss_array_valid, label='Valid Loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
